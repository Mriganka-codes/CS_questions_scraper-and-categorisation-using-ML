{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install the required libraries\n",
        "!pip install transformers pandas openpyxl sklearn\n",
        "\n",
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "# Load the data from the Excel file\n",
        "data = pd.read_excel('/content/2_labeled.xlsx')\n",
        "\n",
        "# Load the pre-trained BERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Tokenize and encode the input text\n",
        "inputs = tokenizer(data['Question'].tolist(), return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "# Move the input data to the appropriate device (Colab should automatically detect and use a GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "model = model.to(device)\n",
        "\n",
        "# Perform inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# Get the predicted difficulty levels\n",
        "predicted_levels = torch.argmax(logits, dim=-1)\n",
        "\n",
        "# Categorize the questions based on the predicted levels\n",
        "data['Predicted_Difficulty'] = ['easy' if level == 0 else 'medium' if level == 1 else 'hard' for level in predicted_levels]\n",
        "\n",
        "# Convert the actual and predicted difficulty levels to numeric values (Easy=0, Medium=1, Hard=2)\n",
        "actual_levels = [0 if level == 'easy' else 1 if level == 'medium' else 2 for level in data['Difficulty']]\n",
        "predicted_levels = [0 if level == 'easy' else 1 if level == 'medium' else 2 for level in data['Predicted_Difficulty']]\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(actual_levels, predicted_levels)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Calculate precision for each class\n",
        "precision_easy = precision_score(actual_levels, predicted_levels, pos_label=0, average='macro')\n",
        "precision_medium = precision_score(actual_levels, predicted_levels, pos_label=1, average='macro')\n",
        "precision_hard = precision_score(actual_levels, predicted_levels, pos_label=2, average='macro')\n",
        "\n",
        "print(f\"Precision (Easy): {precision_easy}\")\n",
        "print(f\"Precision (Medium): {precision_medium}\")\n",
        "print(f\"Precision (Hard): {precision_hard}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bdyFlkTkkVgD",
        "outputId": "4732e4c1-1f0d-497c-e8f0-241d6828dca8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.3)\n",
            "Collecting sklearn\n",
            "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2753623188405797\n",
            "Precision (Easy): 0.25735294117647056\n",
            "Precision (Medium): 0.25735294117647056\n",
            "Precision (Hard): 0.25735294117647056\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 0) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1396: UserWarning: Note that pos_label (set to 2) is ignored when average != 'binary' (got 'macro'). You may use labels=[pos_label] to specify a single positive class.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('best_model.bin')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "rjcah0RSvFDp",
        "outputId": "b9e47b65-193e-4e1f-b54b-4e2193564b45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3a53c21d-c5c8-4f63-8e39-c7325c7b8d86\", \"best_model.bin\", 438019527)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import metrics, preprocessing\n",
        "import transformers\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Setting up device\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Loading the data\n",
        "df = pd.read_excel(\"/content/2_labeled.xlsx\")\n",
        "df = df[['Question', 'Difficulty']]\n",
        "\n",
        "# Encoding labels\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "df['Difficulty'] = label_encoder.fit_transform(df['Difficulty'])\n",
        "\n",
        "# Key variables\n",
        "MAX_LEN = 200\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "VALID_BATCH_SIZE = 4\n",
        "EPOCHS = 10  # Increased number of epochs\n",
        "LEARNING_RATE = 1e-05\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.questions = dataframe.Question\n",
        "        self.labels = dataframe.Difficulty\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.questions)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        question = str(self.questions[index])\n",
        "        question = \" \".join(question.split())\n",
        "\n",
        "        inputs = self.tokenizer.encode_plus(\n",
        "            question,\n",
        "            None,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            pad_to_max_length=True,\n",
        "            return_token_type_ids=True\n",
        "        )\n",
        "        ids = inputs['input_ids']\n",
        "        mask = inputs['attention_mask']\n",
        "        token_type_ids = inputs[\"token_type_ids\"]\n",
        "\n",
        "        return {\n",
        "            'ids': torch.tensor(ids, dtype=torch.long),\n",
        "            'mask': torch.tensor(mask, dtype=torch.long),\n",
        "            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
        "            'labels': torch.tensor(self.labels[index], dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Splitting the dataset\n",
        "train_size = 0.8\n",
        "train_dataset = df.sample(frac=train_size, random_state=200)\n",
        "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(df.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)\n",
        "\n",
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "               'shuffle': True,\n",
        "               'num_workers': 0\n",
        "               }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n",
        "\n",
        "# Model definition\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 3)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "model = BERTClass()\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "def loss_fn(outputs, labels):\n",
        "    return torch.nn.CrossEntropyLoss()(outputs, labels)\n",
        "\n",
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "# Training function\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    for _, data in enumerate(training_loader, 0):\n",
        "        ids = data['ids'].to(device, dtype=torch.long)\n",
        "        mask = data['mask'].to(device, dtype=torch.long)\n",
        "        token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "        labels = data['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        loss = loss_fn(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if _ % 500 == 0:\n",
        "            print(f'Epoch: {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Validation function\n",
        "def validation():\n",
        "    model.eval()\n",
        "    fin_targets = []\n",
        "    fin_outputs = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(testing_loader, 0):\n",
        "            ids = data['ids'].to(device, dtype=torch.long)\n",
        "            mask = data['mask'].to(device, dtype=torch.long)\n",
        "            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n",
        "            labels = data['labels'].to(device, dtype=torch.long)\n",
        "\n",
        "            outputs = model(ids, mask, token_type_ids)\n",
        "            fin_targets.extend(labels.cpu().detach().numpy().tolist())\n",
        "            fin_outputs.extend(outputs.cpu().detach().numpy().tolist())\n",
        "    return fin_outputs, fin_targets\n",
        "\n",
        "# Training and evaluation loop\n",
        "best_accuracy = 0\n",
        "current_epoch = 0  # Track the current epoch\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Starting epoch {epoch + 1}/{EPOCHS}\")\n",
        "    train(epoch)\n",
        "    outputs, targets = validation()\n",
        "    predicted_classes = np.argmax(outputs, axis=1)\n",
        "    accuracy = metrics.accuracy_score(targets, predicted_classes)\n",
        "    f1_score_micro = metrics.f1_score(targets, predicted_classes, average='micro')\n",
        "    f1_score_macro = metrics.f1_score(targets, predicted_classes, average='macro')\n",
        "    print(f\"Accuracy Score = {accuracy}\")\n",
        "    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n",
        "    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n",
        "\n",
        "    # Adjust learning rate based on validation loss\n",
        "    scheduler.step(metrics.log_loss(targets, outputs, labels=[0, 1, 2]))\n",
        "\n",
        "    # Save the model if it has the best accuracy so far\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.bin')\n",
        "        print(f\"Saved Best Model with Accuracy: {best_accuracy}\")\n",
        "\n",
        "    current_epoch = epoch + 1  # Update current epoch\n",
        "\n",
        "# Load the best model\n",
        "model.load_state_dict(torch.load('best_model.bin'))\n",
        "print(f\"Training completed. Best model saved after epoch {current_epoch}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gs2ZhM7OqWnz",
        "outputId": "56a7e228-bbe8-40b3-925a-e3144e58c3fd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (138, 2)\n",
            "TRAIN Dataset: (110, 2)\n",
            "TEST Dataset: (28, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting epoch 1/10\n",
            "Epoch: 0, Loss: 0.9870579242706299\n",
            "Accuracy Score = 0.6071428571428571\n",
            "F1 Score (Micro) = 0.6071428571428571\n",
            "F1 Score (Macro) = 0.499047619047619\n",
            "Saved Best Model with Accuracy: 0.6071428571428571\n",
            "Starting epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1, Loss: 1.1072046756744385\n",
            "Accuracy Score = 0.5\n",
            "F1 Score (Micro) = 0.5\n",
            "F1 Score (Macro) = 0.3974358974358975\n",
            "Starting epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2, Loss: 0.9499197006225586\n",
            "Accuracy Score = 0.6428571428571429\n",
            "F1 Score (Micro) = 0.6428571428571429\n",
            "F1 Score (Macro) = 0.6325281803542673\n",
            "Saved Best Model with Accuracy: 0.6428571428571429\n",
            "Starting epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3, Loss: 0.930146336555481\n",
            "Accuracy Score = 0.6785714285714286\n",
            "F1 Score (Micro) = 0.6785714285714286\n",
            "F1 Score (Macro) = 0.6608695652173914\n",
            "Saved Best Model with Accuracy: 0.6785714285714286\n",
            "Starting epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4, Loss: 0.9137356281280518\n",
            "Accuracy Score = 0.6785714285714286\n",
            "F1 Score (Micro) = 0.6785714285714286\n",
            "F1 Score (Macro) = 0.654978354978355\n",
            "Starting epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 5, Loss: 0.6520323753356934\n",
            "Accuracy Score = 0.75\n",
            "F1 Score (Micro) = 0.75\n",
            "F1 Score (Macro) = 0.7452012383900929\n",
            "Saved Best Model with Accuracy: 0.75\n",
            "Starting epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 6, Loss: 0.6022605299949646\n",
            "Accuracy Score = 0.75\n",
            "F1 Score (Micro) = 0.75\n",
            "F1 Score (Macro) = 0.7485380116959064\n",
            "Starting epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 7, Loss: 0.6034979224205017\n",
            "Accuracy Score = 0.75\n",
            "F1 Score (Micro) = 0.75\n",
            "F1 Score (Macro) = 0.7485380116959064\n",
            "Starting epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 8, Loss: 0.44471749663352966\n",
            "Accuracy Score = 0.6785714285714286\n",
            "F1 Score (Micro) = 0.6785714285714286\n",
            "F1 Score (Macro) = 0.6726522187822498\n",
            "Starting epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 9, Loss: 0.35351794958114624\n",
            "Accuracy Score = 0.7142857142857143\n",
            "F1 Score (Micro) = 0.7142857142857143\n",
            "F1 Score (Macro) = 0.7115009746588695\n",
            "Training completed. Best model saved after epoch 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# Load the model class definition\n",
        "class BERTClass(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERTClass, self).__init__()\n",
        "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.l2 = torch.nn.Dropout(0.3)\n",
        "        self.l3 = torch.nn.Linear(768, 3)\n",
        "\n",
        "    def forward(self, ids, mask, token_type_ids):\n",
        "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
        "        output_2 = self.l2(output_1)\n",
        "        output = self.l3(output_2)\n",
        "        return output\n",
        "\n",
        "# Load the model\n",
        "model = BERTClass()\n",
        "model.load_state_dict(torch.load('best_model.bin'))\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_question(question, tokenizer, max_len=200):\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        question,\n",
        "        None,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_len,\n",
        "        pad_to_max_length=True,\n",
        "        return_token_type_ids=True,\n",
        "        return_attention_mask=True,\n",
        "        truncation=True\n",
        "    )\n",
        "    ids = inputs['input_ids']\n",
        "    mask = inputs['attention_mask']\n",
        "    token_type_ids = inputs['token_type_ids']\n",
        "\n",
        "    return {\n",
        "        'ids': torch.tensor(ids, dtype=torch.long).unsqueeze(0),  # Batch size of 1\n",
        "        'mask': torch.tensor(mask, dtype=torch.long).unsqueeze(0),\n",
        "        'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long).unsqueeze(0)\n",
        "    }\n",
        "\n",
        "# Predict function\n",
        "def predict_difficulty(question, model, tokenizer, label_encoder):\n",
        "    model.eval()\n",
        "    inputs = preprocess_question(question, tokenizer)\n",
        "    ids = inputs['ids'].to(device, dtype=torch.long)\n",
        "    mask = inputs['mask'].to(device, dtype=torch.long)\n",
        "    token_type_ids = inputs['token_type_ids'].to(device, dtype=torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(ids, mask, token_type_ids)\n",
        "        probabilities = torch.softmax(outputs, dim=1).cpu().detach().numpy()\n",
        "        predicted_class = np.argmax(probabilities, axis=1)[0]\n",
        "\n",
        "    return label_encoder.inverse_transform([predicted_class])[0]\n",
        "\n",
        "# Label encoder\n",
        "label_encoder = preprocessing.LabelEncoder()\n",
        "label_encoder.classes_ = np.array(['easy', 'medium', 'hard'])\n",
        "\n",
        "# Example prediction\n",
        "new_question = \"Consider the following schedules involving two transactions. S1 : r1(X) ; r1(Y) ; r2(X) ; r2(Y) ; w2(Y) ; w1(X) S2 : r1(X) ; r2(X) ; r2(Y) ; w2(Y) ; r1(Y) ; w1(X) Which one of the following statements is correct with respect to above? Options: a. Both S1 and S2 are conflict serializable.; b. Both S1 and S2 are not conflict serializable.; c. S1 is conflict serializable and S2 is not conflict serializable.; d. S1 is not conflict serializable and S2 is conflict serializable.\"\n",
        "predicted_difficulty = predict_difficulty(new_question, model, tokenizer, label_encoder)\n",
        "print(f\"The predicted difficulty for the question is: {predicted_difficulty}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uaEWILMGrUiF",
        "outputId": "57209638-f8ef-484f-e448-6e2f3ae882ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The predicted difficulty for the question is: medium\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2699: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}